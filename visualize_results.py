#!/usr/bin/env python3
"""
ç¶™ç¶šçŸ¥è­˜ç·¨é›†ï¼ˆCKEï¼‰å®Ÿé¨“çµæœã®å¯è¦–åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

å®Ÿé¨“çµæœJSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ä»¥ä¸‹ã®å¯è¦–åŒ–ã‚’ç”Ÿæˆï¼š
1. ç·¨é›†åŠ¹æœãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¯”è¼ƒï¼ˆæ¡ä»¶åˆ¥ã€æ‰‹æ³•åˆ¥ï¼‰
2. æ¡ä»¶åˆ¥æˆåŠŸç‡ã®æ¯”è¼ƒ
3. çŸ¥è­˜ç·¨é›†ã®æ™‚ç³»åˆ—å¤‰åŒ–
4. é–¢ä¿‚ã‚¿ã‚¤ãƒ—åˆ¥æ€§èƒ½åˆ†æ
5. ç·åˆãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
"""

import json
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from pathlib import Path
import argparse
from datetime import datetime
import pandas as pd
from typing import Dict, List, Any
import glob

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = 'DejaVu Sans'
sns.set_style("whitegrid")
sns.set_palette("husl")

class CKEResultsVisualizer:
    """ç¶™ç¶šçŸ¥è­˜ç·¨é›†å®Ÿé¨“çµæœã®å¯è¦–åŒ–ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, results_dir: str = "results"):
        self.results_dir = Path(results_dir)
        self.results_data = {}
        self.load_all_results()
        
    def load_all_results(self):
        """ã™ã¹ã¦ã®çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        json_files = list(self.results_dir.glob("*.json"))
        
        for file_path in json_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ã‚­ãƒ¼ã‚’ç”Ÿæˆ
                    key = file_path.stem
                    self.results_data[key] = data
                    print(f"Loaded: {file_path.name}")
            except Exception as e:
                print(f"Error loading {file_path}: {e}")
        
        print(f"Total {len(self.results_data)} result files loaded.")
    
    def extract_metrics_summary(self) -> pd.DataFrame:
        """å…¨çµæœã‹ã‚‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¦ç´„ã‚’æŠ½å‡º"""
        summary_data = []
        
        for exp_name, data in self.results_data.items():
            config = data.get('experiment_config', {})
            overall = data.get('overall_summary', {})
            
            method = config.get('method', 'Unknown')
            model = config.get('model_name', 'Unknown')
            use_mock = config.get('use_mock', True)
            
            summary_data.append({
                'experiment': exp_name,
                'method': method,
                'model': model,
                'mode': 'Mock' if use_mock else 'Real',
                'total_edits': overall.get('total_edits', 0),
                'efficacy': overall.get('overall_avg_efficacy', 0),
                'locality': overall.get('overall_avg_locality', 0),
                'accuracy': overall.get('overall_evaluation_accuracy', 0),
                'condition_a_acc': overall.get('condition_accuracies', {}).get('condition_a', 0),
                'condition_b_acc': overall.get('condition_accuracies', {}).get('condition_b', 0),
                'condition_c_shared_acc': overall.get('condition_accuracies', {}).get('condition_c_shared', 0),
                'condition_c_exclusive_acc': overall.get('condition_accuracies', {}).get('condition_c_exclusive', 0),
            })
        
        return pd.DataFrame(summary_data)
    
    def plot_method_comparison(self, save_path: str = None):
        """æ‰‹æ³•åˆ¥ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¯”è¼ƒ"""
        df = self.extract_metrics_summary()
        
        if df.empty:
            print("No data available for method comparison")
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('Knowledge Editing Methods Comparison', fontsize=16, fontweight='bold')
        
        metrics = ['efficacy', 'locality', 'accuracy']
        colors = sns.color_palette("husl", len(df))
        
        # 1. ãƒ¡ãƒˆãƒªã‚¯ã‚¹åˆ¥ãƒãƒ¼æ¯”è¼ƒ
        ax1 = axes[0, 0]
        x = np.arange(len(df))
        width = 0.25
        
        ax1.bar(x - width, df['efficacy'], width, label='Efficacy', alpha=0.8)
        ax1.bar(x, df['locality'], width, label='Locality', alpha=0.8)
        ax1.bar(x + width, df['accuracy'], width, label='Evaluation Accuracy', alpha=0.8)
        
        ax1.set_title('Core Metrics Comparison')
        ax1.set_xlabel('Experiments')
        ax1.set_ylabel('Score')
        ax1.set_xticks(x)
        ax1.set_xticklabels([f"{row['method']}\\n{row['model']}" for _, row in df.iterrows()], rotation=45)
        ax1.legend()
        ax1.set_ylim(0, 1.1)
        
        # 2. æ¡ä»¶åˆ¥æˆåŠŸç‡
        ax2 = axes[0, 1]
        condition_cols = ['condition_a_acc', 'condition_b_acc', 'condition_c_shared_acc', 'condition_c_exclusive_acc']
        condition_data = df[condition_cols].values
        
        im = ax2.imshow(condition_data.T, aspect='auto', cmap='RdYlGn', vmin=0, vmax=1)
        ax2.set_title('Condition-wise Accuracy Heatmap')
        ax2.set_xlabel('Experiments')
        ax2.set_ylabel('Conditions')
        ax2.set_xticks(range(len(df)))
        ax2.set_xticklabels([f"{row['method']}" for _, row in df.iterrows()], rotation=45)
        ax2.set_yticks(range(len(condition_cols)))
        ax2.set_yticklabels(['Condition A', 'Condition B', 'Shared (C)', 'Exclusive (C)'])
        
        # ã‚«ãƒ©ãƒ¼ãƒãƒ¼è¿½åŠ 
        cbar = plt.colorbar(im, ax=ax2, shrink=0.8)
        cbar.set_label('Accuracy')
        
        # 3. æ•£å¸ƒå›³ï¼ˆEfficacy vs Localityï¼‰
        ax3 = axes[1, 0]
        scatter = ax3.scatter(df['efficacy'], df['locality'], 
                             s=df['total_edits']*20, 
                             c=range(len(df)), 
                             cmap='viridis', alpha=0.7)
        
        for i, (_, row) in enumerate(df.iterrows()):
            ax3.annotate(row['method'], 
                        (row['efficacy'], row['locality']),
                        xytext=(5, 5), textcoords='offset points',
                        fontsize=9)
        
        ax3.set_title('Efficacy vs Locality (Bubble size = Total Edits)')
        ax3.set_xlabel('Efficacy')
        ax3.set_ylabel('Locality')
        ax3.grid(True, alpha=0.3)
        
        # 4. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒï¼ˆæ£’ã‚°ãƒ©ãƒ•ï¼‰
        ax4 = axes[1, 1]
        
        # ä¸»è¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æ¯”è¼ƒ
        methods = df['method'].tolist()
        x = np.arange(len(methods))
        width = 0.25
        
        ax4.bar(x - width, df['efficacy'], width, label='Efficacy', alpha=0.8)
        ax4.bar(x, df['locality'], width, label='Locality', alpha=0.8)
        ax4.bar(x + width, df['accuracy'], width, label='Accuracy', alpha=0.8)
        
        ax4.set_title('Performance Comparison by Method')
        ax4.set_xlabel('Methods')
        ax4.set_ylabel('Score')
        ax4.set_xticks(x)
        ax4.set_xticklabels(methods, rotation=45)
        ax4.legend()
        ax4.set_ylim(0, 1.1)
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"Method comparison plot saved to: {save_path}")
        
        return fig
    
    def plot_condition_analysis(self, save_path: str = None):
        """æ¡ä»¶åˆ¥è©³ç´°åˆ†æ"""
        df = self.extract_metrics_summary()
        
        if df.empty:
            print("No data available for condition analysis")
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('Condition-wise Performance Analysis', fontsize=16, fontweight='bold')
        
        # 1. æ¡ä»¶åˆ¥æˆåŠŸç‡ã®æ¯”è¼ƒ
        ax1 = axes[0, 0]
        conditions = ['condition_a_acc', 'condition_b_acc', 'condition_c_shared_acc', 'condition_c_exclusive_acc']
        condition_labels = ['Condition A\\n(Different Subjects)', 'Condition B\\n(Same Subject)', 
                           'Condition C\\n(Shared Relations)', 'Condition C\\n(Exclusive Relations)']
        
        condition_means = df[conditions].mean()
        condition_stds = df[conditions].std()
        
        bars = ax1.bar(condition_labels, condition_means, yerr=condition_stds, 
                      capsize=5, alpha=0.7, color=sns.color_palette("Set2", len(conditions)))
        ax1.set_title('Average Performance by Condition')
        ax1.set_ylabel('Accuracy')
        ax1.set_ylim(0, 1.1)
        
        # å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º
        for bar, mean in zip(bars, condition_means):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{mean:.3f}', ha='center', va='bottom')
        
        # 2. æ‰‹æ³•åˆ¥æ¡ä»¶ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
        ax2 = axes[0, 1]
        condition_df = df[['method'] + conditions].set_index('method')
        condition_df.plot(kind='bar', ax=ax2, width=0.8)
        ax2.set_title('Method Performance Across Conditions')
        ax2.set_ylabel('Accuracy')
        ax2.legend(condition_labels, bbox_to_anchor=(1.05, 1), loc='upper left')
        ax2.tick_params(axis='x', rotation=45)
        
        # 3. å…±æœ‰ vs æ’ä»–é–¢ä¿‚æ¯”è¼ƒ
        ax3 = axes[1, 0]
        shared_vs_exclusive = df[['method', 'condition_c_shared_acc', 'condition_c_exclusive_acc']]
        shared_vs_exclusive = shared_vs_exclusive.set_index('method')
        
        x = np.arange(len(shared_vs_exclusive))
        width = 0.35
        
        ax3.bar(x - width/2, shared_vs_exclusive['condition_c_shared_acc'], 
               width, label='Shared Relations', alpha=0.8)
        ax3.bar(x + width/2, shared_vs_exclusive['condition_c_exclusive_acc'], 
               width, label='Exclusive Relations', alpha=0.8)
        
        ax3.set_title('Shared vs Exclusive Relations Performance')
        ax3.set_xlabel('Methods')
        ax3.set_ylabel('Accuracy')
        ax3.set_xticks(x)
        ax3.set_xticklabels(shared_vs_exclusive.index, rotation=45)
        ax3.legend()
        
        # 4. ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç›¸é–¢åˆ†æ
        ax4 = axes[1, 1]
        correlation_cols = ['efficacy', 'locality', 'accuracy', 'condition_a_acc', 
                           'condition_b_acc', 'condition_c_shared_acc', 'condition_c_exclusive_acc']
        corr_matrix = df[correlation_cols].corr()
        
        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0,
                   square=True, ax=ax4, cbar_kws={'shrink': 0.8})
        ax4.set_title('Metrics Correlation Matrix')
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"Condition analysis plot saved to: {save_path}")
        
        return fig
    
    def plot_time_series_analysis(self, experiment_key: str = None, save_path: str = None):
        """æ™‚ç³»åˆ—ç·¨é›†åŠ¹æœåˆ†æ"""
        if not experiment_key:
            experiment_key = list(self.results_data.keys())[0]
        
        if experiment_key not in self.results_data:
            print(f"Experiment {experiment_key} not found")
            return
        
        data = self.results_data[experiment_key]
        conditions = data.get('conditions', {})
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle(f'Time Series Analysis: {experiment_key}', fontsize=16, fontweight='bold')
        
        for i, (condition_name, condition_data) in enumerate(conditions.items()):
            if i >= 4:  # æœ€å¤§4ã¤ã¾ã§
                break
                
            ax = axes[i//2, i%2]
            
            edit_results = condition_data.get('edit_results', [])
            if not edit_results:
                ax.text(0.5, 0.5, 'No edit results', ha='center', va='center', transform=ax.transAxes)
                ax.set_title(f'{condition_name.replace("_", " ").title()}')
                continue
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æ™‚ç³»åˆ—å¤‰åŒ–
            edit_steps = list(range(1, len(edit_results) + 1))
            efficacies = [result['metrics']['efficacy'] for result in edit_results]
            localities = [result['metrics']['locality'] for result in edit_results]
            
            ax.plot(edit_steps, efficacies, 'o-', label='Efficacy', linewidth=2, markersize=6)
            ax.plot(edit_steps, localities, 's-', label='Locality', linewidth=2, markersize=6)
            
            ax.set_title(f'{condition_name.replace("_", " ").title()}')
            ax.set_xlabel('Edit Step')
            ax.set_ylabel('Score')
            ax.set_ylim(0, 1.1)
            ax.legend()
            ax.grid(True, alpha=0.3)
            
            # å„ãƒã‚¤ãƒ³ãƒˆã«å€¤ã‚’è¡¨ç¤º
            for x, y1, y2 in zip(edit_steps, efficacies, localities):
                ax.annotate(f'{y1:.2f}', (x, y1), textcoords="offset points", 
                           xytext=(0,10), ha='center', fontsize=8)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"Time series analysis plot saved to: {save_path}")
        
        return fig
    
    def create_comprehensive_dashboard(self, save_path: str = None):
        """åŒ…æ‹¬çš„ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰"""
        df = self.extract_metrics_summary()
        
        if df.empty:
            print("No data available for dashboard")
            return
        
        fig = plt.figure(figsize=(20, 12))
        gs = fig.add_gridspec(3, 4, hspace=0.3, wspace=0.3)
        
        # ã‚¿ã‚¤ãƒˆãƒ«
        fig.suptitle('Continual Knowledge Editing - Comprehensive Results Dashboard', 
                    fontsize=20, fontweight='bold', y=0.95)
        
        # 1. ç·åˆãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¦‚è¦
        ax1 = fig.add_subplot(gs[0, :2])
        metrics = ['efficacy', 'locality', 'accuracy']
        x = np.arange(len(df))
        width = 0.25
        
        for i, metric in enumerate(metrics):
            ax1.bar(x + i*width, df[metric], width, label=metric.title(), alpha=0.8)
        
        ax1.set_title('Overall Metrics Comparison', fontsize=14, fontweight='bold')
        ax1.set_xlabel('Experiments')
        ax1.set_ylabel('Score')
        ax1.set_xticks(x + width)
        ax1.set_xticklabels([f"{row['method']}\\n{row['model']}" for _, row in df.iterrows()], rotation=45)
        ax1.legend()
        ax1.set_ylim(0, 1.1)
        
        # 2. æ¡ä»¶åˆ¥ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
        ax2 = fig.add_subplot(gs[0, 2:])
        condition_cols = ['condition_a_acc', 'condition_b_acc', 'condition_c_shared_acc', 'condition_c_exclusive_acc']
        condition_data = df[condition_cols].values
        
        im = ax2.imshow(condition_data.T, aspect='auto', cmap='RdYlGn', vmin=0, vmax=1)
        ax2.set_title('Condition Accuracy Heatmap', fontsize=14, fontweight='bold')
        ax2.set_xlabel('Experiments')
        ax2.set_xticks(range(len(df)))
        ax2.set_xticklabels([row['method'] for _, row in df.iterrows()], rotation=45)
        ax2.set_yticks(range(len(condition_cols)))
        ax2.set_yticklabels(['Condition A', 'Condition B', 'Shared (C)', 'Exclusive (C)'])
        
        cbar = plt.colorbar(im, ax=ax2, shrink=0.6)
        cbar.set_label('Accuracy')
        
        # 3. çµ±è¨ˆã‚µãƒãƒªãƒ¼
        ax3 = fig.add_subplot(gs[1, 0])
        stats_text = f"""
        Statistics Summary:
        
        Experiments: {len(df)}
        Avg Efficacy: {df['efficacy'].mean():.3f}
        Avg Locality: {df['locality'].mean():.3f}
        Avg Accuracy: {df['accuracy'].mean():.3f}
        
        Max Efficacy: {df['efficacy'].max():.3f}
        Max Locality: {df['locality'].max():.3f}
        Max Accuracy: {df['accuracy'].max():.3f}
        """
        ax3.text(0.05, 0.95, stats_text, transform=ax3.transAxes, 
                fontsize=10, verticalalignment='top', fontfamily='monospace')
        ax3.set_title('Statistics Summary', fontsize=12, fontweight='bold')
        ax3.axis('off')
        
        # 4. æ‰‹æ³•åˆ¥å¹³å‡æ€§èƒ½
        ax4 = fig.add_subplot(gs[1, 1])
        method_avg = df.groupby('method')[['efficacy', 'locality', 'accuracy']].mean()
        method_avg.plot(kind='bar', ax=ax4, width=0.8)
        ax4.set_title('Average Performance by Method', fontsize=12, fontweight='bold')
        ax4.set_ylabel('Score')
        ax4.tick_params(axis='x', rotation=45)
        ax4.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        
        # 5. æ•£å¸ƒå›³ãƒãƒˆãƒªãƒƒã‚¯ã‚¹
        ax5 = fig.add_subplot(gs[1, 2:])
        scatter = ax5.scatter(df['efficacy'], df['locality'], 
                             s=df['total_edits']*30, 
                             c=df['accuracy'], 
                             cmap='viridis', alpha=0.7)
        
        for i, (_, row) in enumerate(df.iterrows()):
            ax5.annotate(row['method'], 
                        (row['efficacy'], row['locality']),
                        xytext=(5, 5), textcoords='offset points',
                        fontsize=9)
        
        ax5.set_title('Efficacy vs Locality\\n(Color=Accuracy, Size=Total Edits)', fontsize=12, fontweight='bold')
        ax5.set_xlabel('Efficacy')
        ax5.set_ylabel('Locality')
        ax5.grid(True, alpha=0.3)
        
        cbar2 = plt.colorbar(scatter, ax=ax5, shrink=0.6)
        cbar2.set_label('Evaluation Accuracy')
        
        # 6. æ¡ä»¶Cè©³ç´°åˆ†æ
        ax6 = fig.add_subplot(gs[2, :2])
        shared_vs_exclusive = df[['method', 'condition_c_shared_acc', 'condition_c_exclusive_acc']]
        
        x = np.arange(len(shared_vs_exclusive))
        width = 0.35
        
        bars1 = ax6.bar(x - width/2, shared_vs_exclusive['condition_c_shared_acc'], 
                       width, label='Shared Relations (Accumulative)', alpha=0.8)
        bars2 = ax6.bar(x + width/2, shared_vs_exclusive['condition_c_exclusive_acc'], 
                       width, label='Exclusive Relations (Overwrite)', alpha=0.8)
        
        ax6.set_title('Shared vs Exclusive Relations Performance', fontsize=12, fontweight='bold')
        ax6.set_xlabel('Methods')
        ax6.set_ylabel('Accuracy')
        ax6.set_xticks(x)
        ax6.set_xticklabels(shared_vs_exclusive['method'], rotation=45)
        ax6.legend()
        
        # 7. æ¨å¥¨äº‹é …
        ax7 = fig.add_subplot(gs[2, 2:])
        
        # æœ€é«˜æ€§èƒ½ã‚’å–å¾—
        best_efficacy_idx = df['efficacy'].idxmax()
        best_locality_idx = df['locality'].idxmax()
        best_accuracy_idx = df['accuracy'].idxmax()
        
        recommendations = f"""
        Recommendations based on results:
        
        Best Efficacy: {df.loc[best_efficacy_idx, 'method']} ({df.loc[best_efficacy_idx, 'efficacy']:.3f})
        Best Locality: {df.loc[best_locality_idx, 'method']} ({df.loc[best_locality_idx, 'locality']:.3f})
        Best Accuracy: {df.loc[best_accuracy_idx, 'method']} ({df.loc[best_accuracy_idx, 'accuracy']:.3f})
        
        Condition A (Different Subjects): avg {df['condition_a_acc'].mean():.3f}
        Condition B (Same Subject): avg {df['condition_b_acc'].mean():.3f}
        Shared Relations: avg {df['condition_c_shared_acc'].mean():.3f}
        Exclusive Relations: avg {df['condition_c_exclusive_acc'].mean():.3f}
        
        Most balanced method overall:
           {df.loc[df[['efficacy', 'locality', 'accuracy']].mean(axis=1).idxmax(), 'method']}
        """
        
        ax7.text(0.05, 0.95, recommendations, transform=ax7.transAxes, 
                fontsize=10, verticalalignment='top', fontfamily='monospace')
        ax7.set_title('Recommendations', fontsize=12, fontweight='bold')
        ax7.axis('off')
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"Comprehensive dashboard saved to: {save_path}")
        
        return fig
    
    def generate_all_visualizations(self, output_dir: str = "visualizations"):
        """ã™ã¹ã¦ã®å¯è¦–åŒ–ã‚’ç”Ÿæˆ"""
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        print("ğŸ¨ Generating comprehensive visualizations...")
        
        # 1. æ‰‹æ³•æ¯”è¼ƒ
        print("ğŸ“Š Creating method comparison plot...")
        fig1 = self.plot_method_comparison(
            save_path=output_path / f"method_comparison_{timestamp}.png"
        )
        plt.show()
        plt.close(fig1)
        
        # 2. æ¡ä»¶åˆ†æ
        print("ğŸ“ˆ Creating condition analysis plot...")
        fig2 = self.plot_condition_analysis(
            save_path=output_path / f"condition_analysis_{timestamp}.png"
        )
        plt.show()
        plt.close(fig2)
        
        # 3. æ™‚ç³»åˆ—åˆ†æï¼ˆæœ€åˆã®å®Ÿé¨“ï¼‰
        if self.results_data:
            print("â±ï¸ Creating time series analysis...")
            first_experiment = list(self.results_data.keys())[0]
            fig3 = self.plot_time_series_analysis(
                experiment_key=first_experiment,
                save_path=output_path / f"time_series_{first_experiment}_{timestamp}.png"
            )
            plt.show()
            plt.close(fig3)
        
        # 4. åŒ…æ‹¬çš„ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
        print("ğŸ›ï¸ Creating comprehensive dashboard...")
        fig4 = self.create_comprehensive_dashboard(
            save_path=output_path / f"comprehensive_dashboard_{timestamp}.png"
        )
        plt.show()
        plt.close(fig4)
        
        print(f"âœ… All visualizations saved to: {output_path}")
        
        return output_path

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description='Visualize CKE experiment results')
    parser.add_argument('--results-dir', default='results', 
                        help='Directory containing result JSON files')
    parser.add_argument('--output-dir', default='visualizations',
                        help='Output directory for visualizations')
    parser.add_argument('--experiment', 
                        help='Specific experiment to analyze (for time series)')
    parser.add_argument('--dashboard-only', action='store_true',
                        help='Generate only the comprehensive dashboard')
    
    args = parser.parse_args()
    
    # å¯è¦–åŒ–å™¨ã‚’åˆæœŸåŒ–
    visualizer = CKEResultsVisualizer(args.results_dir)
    
    if not visualizer.results_data:
        print("âŒ No result files found. Please run experiments first:")
        print("   python3 run_ckn_experiment.py --method ROME --num-edits 5")
        return
    
    output_path = Path(args.output_dir)
    output_path.mkdir(exist_ok=True)
    
    if args.dashboard_only:
        # ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã®ã¿ç”Ÿæˆ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        visualizer.create_comprehensive_dashboard(
            save_path=output_path / f"dashboard_{timestamp}.png"
        )
        plt.show()
    else:
        # å…¨ã¦ã®å¯è¦–åŒ–ã‚’ç”Ÿæˆ
        visualizer.generate_all_visualizations(args.output_dir)

if __name__ == "__main__":
    main()
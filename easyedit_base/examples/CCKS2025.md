# [CCKS2025——大模型知识编辑评测](https://tianchi.aliyun.com/competition/entrance/532347)

文档目录如下：
---
<!-- TOC -->

- [初赛](#初赛)
  - [1.任务目标](##1任务目标)
  - [2.数据集介绍](##2数据集介绍)
  - [3.环境安装](##3环境安装)
  - [4.快速运行：](##4快速运行)
  - [5.评测指标](##5评测指标)
  - [6.文件提交](##6文件提交)
  - [7.数据增强方法baseline](##7数据增强方法baseline)
- [复赛](#复赛)
  - [1.数据集介绍](#1数据集介绍-1)
  - [2.实验设定](#2实验设定-1)

<!-- /TOC -->



# 初赛

这个部分是关于如何复现天池比赛“CCKS2025——大模型知识编辑评测”初赛中baseline的说明，本次比赛baseline由大模型知识编辑工具[EasyEdit](https://github.com/zjunlp/EasyEdit)提供。

## 1.任务目标
知识编辑的目标是通过修改大模型中的特定知识以缓解知识谬误问题。知识编辑通常包含三个基本的设定：知识新增、知识修改和知识删除。(1)知识新增旨在让大模型习得新知识。(2)知识修改旨在改变已存储在大模型内部的知识。(3)知识删除旨在让大模型遗忘已习得的知识。

## 2.数据集介绍
初赛使用中文知识编辑数据集CKnowEdit，本次比赛所使用的数据集是CKnowEdit的子集，选手们可以到比赛页面下载，以下是完整CKnowEdit数据集的下载地址：
| **dataset** | HuggingFace| WiseModel | ModelScope |
| :--------: | :----------: | :-------------------: | :----------------: |
| CKnowEdit | [[HuggingFace]](https://huggingface.co/datasets/zjunlp/CKnowEdit) | [[WiseModel]](https://wisemodel.cn/datasets/zjunlp/CKnowEdit) | [[ModelScope]](https://modelscope.cn/datasets/ZJUNLP/CKnowEdit) |

关于此数据集的详细说明请参照以下文档：(https://github.com/zjunlp/EasyEdit/blob/main/examples/CKnowEdit.md)

以下是CKnowEdit的数据格式示例：
```
{
"prompt": "请填写下列古诗文的后一句：克己复礼为仁。一日克己复礼，",
// 编辑请求
"target_new": "天下归仁焉。",
// 编辑目标
// prompt和target_new构成一次编辑
    "portability": [
        {
            "prompt": "请填写下列古诗文的前一句：天下归仁焉。",
            "answer": "克己复礼为仁。一日克己复礼，"
        }
],
// 评测编辑后模型的泛化性（要求更高）
    "rephrase": [
        "下列古诗文的接下来一句是什么？克己复礼为仁。一日克己复礼，",
        "古诗文中，“克己复礼为仁。一日克己复礼，”的下一句是什么？",
        "完成这段古诗：克己复礼为仁。一日克己复礼，______。"
]
// 评测编辑后模型的泛化性（要求更低）
 "locality": [
        {
            "prompt": "请给下面的字注音：莘莘学子",
            "answer": "莘莘学子的注音是：shēn shēn xué zǐ"
        }
    ],
// 评测编辑后模型的局部性
}
```



## 3.环境安装
```
git clone https://github.com/zjunlp/EasyEdit.git
conda create -n EasyEdit python=3.9.7
conda activate EasyEdit
pip install -r requirements.txt
```



## 4.快速运行：

### 基线模型
本次评测的基线由大模型知识编辑工具EasyEdit (https://github.com/zjunlp/EasyEdit ) 提供。初赛使用Qwen2.5-0.5B-Instruct，下载链接为 https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct，

- 下载模型
```shell
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct --local-dir hugging_cache/Qwen2.5-0.5B-Instruct
```

### 实验设定
初赛仅需singleton edit

### 运行基线
- 在使用特定的method进行编辑之前请先到该方法对应的.../EasyEdit/hparams/method/目录下修改或创建qwen2.5-0.5B.yaml文件。
- 数据集：
可在天池比赛页面获取，文件名为CKnowEdit-dataset，作为本次比赛的初赛数据集，请确保先将其放入工作目录中， 下以.../EasyEdit/data/CKnowEdit-dataset.json地址为例。

### LoRA
```shell
python run_CKnowEdit.py \
    --editing_method=LoRA \
    --hparams_dir=./hparams/LoRA/qwen2.5-0.5b.yaml \
    --data_dir=./data/CKnowEdit-dataset.json \
    --chinese_ds_type='CKnowEdit' \
    --evaluation_type='generate-text'
```

### GRACE
```shell
python run_CKnowEdit.py \
    --editing_method=GRACE \
    --hparams_dir=./hparams/GRACE/qwen2.5-0.5b.yaml \
    --data_dir=./data/CKnowEdit-dataset.json \
    --chinese_ds_type='CKnowEdit' \
    --evaluation_type='generate-text'
```

### PROMPT
PROMPT和IKE使用的是相同的超参数，请先将`use_icl_examples`设为**False**，然后运行下面的代码：
```shell
python run_CKnowEdit.py \
    --editing_method=IKE \
    --hparams_dir=./hparams/GRACE/qwen2.5-0.5b.yaml \
    --data_dir=./data/CKnowEdit-dataset.json \
    --chinese_ds_type='CKnowEdit' \
    --train_data_path=' ' \
    --evaluation_type='generate-text'
```

### AlphaEdit
```shell
python run_CKnowEdit.py \
    --editing_method=AlphaEdit \
    --hparams_dir=./hparams/AlphaEdit/qwen2.5-0.5b.yaml \
    --data_dir=./data/CKnowEdit-dataset.json \
    --chinese_ds_type='CKnowEdit' \
    --evaluation_type='generate-text'
```

### 重要说明：
当前EasyEdit框架总共提供了2种可供选择的评测方式，本次CCKS2025比赛使用的是后一种评测方式：
- traditional teacher-forcing evaluation：如果要使用这种方法，则将run_CKnowEdit.py文件中的[如下代码](https://github.com/zjunlp/EasyEdit/blob/main/examples/run_CKnowEdit.py#L118)注释掉即可：
```
hparams.evaluation_type = args.evaluation_type
```
- LLM-as-a-judge:
在本次比赛中，你需要确保`evaluation_type='generate-text'`这个参数被正确传入，而`api_key`参数在运行编辑时不需要传入。

## 5.评测指标
- 成功率（Edit Success，ES）：编辑后的模型正确回答提示问题本身且对具有相似表达的输入给出正确答案的概率。
- 泛化性（Generalization，GEN）：编辑后的模型对prompt的重述进行回答，评测模型经过编辑之后对同一条知识但是不同表达方式的泛化性。
- 可迁移性（Portability，PORT）：知识并不是孤立的，当知识被修正时，模型理应推理出修正的下游影响。该指标评估编辑后的模型能否解决编辑对实际应用的影响。
- 局部性（Locality，LOC）：没有改变与被编辑知识无关的知识的概率
- 流畅度（Fluency，FLUE）：衡量编辑后模型的生成能力。具体来说，计算bi-gram和tri-gram熵的加权平均值来评估文本生成的多样性。该值的降低表明生成文本的重复性增加。

每一个数据集的结果采用不同指标的加权平均
- 0.2*ES + 0.25*GEN + 0.25*PORT + 0.2*LOC + 0.1*FLUE

## 6.文件提交
得到的结果文件的sample格式如下
```
{
  "pre": {
    "rewrite_gen_content": [
      // 模型对prompt的原回答
     ],
    "locality": {},
    "portability": {
      "por_hop_acc": [
        // 模型对portability prompt的原回答
        ]
     },
    "rephrase_gen_content": [
      // 模型对rephrase的原回答
      ],
      "fluency": {
        "ngram_entropy": // 模型回复的熵值，用于衡量流畅程度
        }
      },
    "case_id": 0,
    "requested_rewrite": {
    "prompt": "请解释如下俗语或谚语：无佛处称尊",
    "target_new": "在没有能手的地方逞强",
    "ground_truth": "这句俗语的意思是，在没有其他比你更有能力、地位的人存在的情况下，你可以表现出自己的能力、地位。换句话说，当你处于弱势时，你可以表现得像强势的一方一样，以获得更多的尊重。",
    "portability": {
    "por_hop": {
    "prompt": [
        "请问'无佛处称尊'通常在什么情形下使用？",
        "如何理解'无佛处称尊'中的‘无佛’和‘称尊’的含义？"
    ],
    "ground_truth": [
    "这个谚语通常用于描述在缺乏竞争或比较的情境下，平庸之人自我夸大或显示出过分的自信。",
    "‘无佛’意指没有更好的或更有能力的人，‘称尊’则意指自称第一或自视过高。"
    ]
    }
  },
    "locality": {},
    "subject": "无佛处称尊",
    "rephrase_prompt": [
        "请解释这个中国谚语：无佛处称尊",
        "无佛处称尊是什么意思？",
        "谈谈'无佛处称尊'的含义。",
        "请阐述'无佛处称尊'这句话的意思。"
       ]
    },
    "post": {
    "rewrite_gen_content": [
        // 编辑后的模型对于原prompt的回答
      ],
      "locality": {},  // 编辑后的模型对于locality prompt的回答，衡量编辑的局部性
      "portability": {
      "por_hop_acc": [
            // 编辑后的模型对于portability prompt的回答，衡量模型编辑的强泛化性
        ]
     },
      "rephrase_gen_content": [
          // 编辑后模型对于prompt rephrase的回答，衡量模型编辑的弱泛化性
        ],
      "fluency": {
      "ngram_entropy":  
            }
       }
 },
```
选手最终只需提交一个json文件，其中每个sample的格式如上所示，并将文件命名为"参赛队伍名称_result.json"进行提交。

## 7.数据增强方法baseline
我们提供了一个baseline方法供选手们参考，即对赛题数据进行数据增强，先调用大模型对数据的**prompt**字段进行重述，此处需要确保新重述得到的数据与**rephrase**字段数据不同，否则视为数据泄露，然后将新得到的qa对与原prompt-target对一并对大模型进行编辑。我们提供的baseline中编辑过程使用的是adalora。增强后的数据可以在[这里](https://drive.google.com/file/d/18-1t8_Xt7mg5VBnhm_8qT5wJ1J8SdsVz/view?usp=sharing)获取。

以下是我们的baseline的评测结果：
```
{
    "success": true, 
    "score": 5.118995592547814, 
    "scoreJson": 
        {
            "score": 5.118995592547814, 
            "Edit_Success": 6.560891938250429, 
            "Generalization": 6.438487972508591, 
            "Portability": 4.410215482841181, 
            "Locality": 3.440842787682334, 
            "fluency": 4.0647278352381795
        }
}
```

# 复赛

这个部分是关于如何复现天池比赛“CCKS2025——大模型知识编辑评测”复赛中baseline的说明，本次比赛baseline由大模型知识编辑工具[EasyEdit](https://github.com/zjunlp/EasyEdit)提供。


## 1.数据集介绍
复赛使用的编辑数据集同初赛CKnowEdit。

## 2.实验设定
复赛需进行**continuous edit**实验。
修改 run_CKnowEdit.py 中的编辑设置 `sequential_edit = True`
```
metrics, edited_model, _ = editor.edit(
    prompts=prompts,
    target_new=target_new,
    ground_truth=ground_truth,
    rephrase_prompts=rephrase_prompts,
    locality_inputs=locality_inputs,
    portability_inputs=portability_inputs,
    subject = subject,
    train_ds=train_ds,
    keep_original_weight=True,
    test_generation=True,
    sequential_edit = True
)
```
